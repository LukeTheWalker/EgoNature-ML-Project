<!DOCTYPE html>
<html>
<head>
<title>relazione.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="relazione-progetto-di-machine-learning">Relazione Progetto di Machine Learning</h1>
<p><strong>Nome</strong>: <em>Luca Venerando</em></p>
<p><strong>Cognome</strong>: <em>Greco</em></p>
<p><strong>Matricola</strong>: <em>1000016165</em></p>
<p><strong>Corso</strong>: Machine Learning</p>
<p><strong>Docente</strong>: Giovanni Maria Farinella</p>
<p><strong>Anno Accademico</strong>: <em>2022/2023</em></p>
<h2 id="indice">Indice</h2>
<ol>
<li><a href="#problema">Problema</a></li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#metodi">Metodi</a></li>
<li><a href="#valutazione">Valutazione</a></li>
<li><a href="#esperimenti">Esperimenti</a></li>
<li><a href="#demo">Demo</a></li>
<li><a href="#codice">Codice</a></li>
<li><a href="#conclusioni">Conclusioni</a></li>
</ol>
<h2 id="problema">Problema</h2>
<p>[Extracted from the EgoNature website]</p>
<p>Localizzare i visitatori in ambienti naturali rappresenta una sfida a causa dell'indisponibilità di telecamere preinstallate o di altre infrastrutture come reti WiFi.</p>
<p>La localizzazione può essere utile per fornire servizi sia ai visitatori (ad esempio, mostrando dove si trovano o cosa vedere successivamente) che al gestore del sito (ad esempio, per capire a cosa i visitatori prestano più attenzione e cosa perdono durante le loro visite).</p>
<p>Consideriamo la localizzazione del visitatore del sito naturale come un problema di classificazione. In particolare, si esplorerà un approccio di classificazione basato su immagini.</p>
<p>Ciascuno dei modelli considerati è addestrato e valutato sul dataset EgoNature secondo i tre livelli definiti di granularità della localizzazione: 9 Context, 9 Subcontext e 17 Context-Subcontext</p>
<h2 id="dataset">Dataset</h2>
<p>Il Dataset utilizzato in questo progetto è <a href="https://iplab.dmi.unict.it/EgoNature">Egonaure</a>, un dataset di immagini egocentriche di visitatori all'interno dell'orto botanico di Catania.</p>
<p>Il dataset presenta sia immagini egocentriche che dati GPS dei visitatori, ma per lo sviluppo di questo progetto ci limiteremo a considerare le immagini. Il dataset è diviso in 3 livelli di granularità della localizzazione: 9 Context, 9 Subcontext e 17 Context-Subcontext, I 9 contesti rapresentano 9 macroaree dell'orto, i 9 sub-context sono le sottoaree della macroarea &quot;Giardino Siciliano&quot; e i 17 Context-Subcontext sono le combinazioni dei due livelli precedenti, una mappa esplicativa delle aree:</p>
<p><img src="egonature_images/TopologiaOrto.png" alt="Map"></p>
<p>Le immagini si trovano tutte nella stessa cartella insieme ad alcuni file .txt che contengono le classi di appartenza di ciascuna immagine. Oltre ad essere suddiviso per modality è presente anche una divisione in tre fold e uno split fra training e test set. Per ciascuna combinazione di modality, fold e set è presente un apposito file di txt, il cui filename segue la seguente regola:</p>
<p>[train|test]_[Con|Sub|ConSub]_[0|1|2].txt
dove:</p>
<ul>
<li>[train|test] identifica il set di addestramento o il set di test</li>
<li>[Con|Sub|ConSub] identifica la modality</li>
<li>[0|1|2] identifica il fold</li>
</ul>
<p>Anche le immagini seguono una specifica nomenclatura, in particolare:</p>
<p>yyyy_mm_dd_sessionID_frameID.jpeg</p>
<ul>
<li>yyyy è l'anno di registrazione nel formato a 4 cifre</li>
<li>mm è il mese di registrazione nel formato a 2 cifre</li>
<li>dd è il giorno di registrazione nel formato a 2 cifre</li>
<li>sessionID è un ID incrementale giornaliero per le registrazioni, a partire da 000, in un formato a 3 cifre</li>
<li>frameID identifica un frame del giorno e della sessione corrente</li>
</ul>
<p>Ulteriori informazioni sul dataset sono disponibili nella <a href="https://iplab.dmi.unict.it/EgoNature/dataset.html">documentazione</a> del dataset, di seguito alcune informazioni utili estratte dalla documentazione:</p>
<p>Metodo di acquisizione: I volontari sono stati istruiti a visitare tutti i 9 contesti senza alcuna restrizione specifica, potendo trascorrere il tempo che desideravano in ciascun contesto. Durante la visita, abbiamo chiesto ad ogni volontario di esplorare il sito naturale indossando una telecamera e uno smartphone. La telecamera indossabile, un headset Pupil 3D Eye Tracker, è stata utilizzata per raccogliere video egocentrici delle visite, mentre lo smartphone è stato utilizzato per raccogliere informazioni sulla posizione GPS. I video sono stati acquisiti ad una risoluzione di 1280x720 pixel e una frequenza di 60 fps. Di questi, una selezione di 63,581 frames è stata fatta e successivamente downscalata.</p>
<p>Un estratto del dataset:</p>
<p><img src="egonature_images/datasetexample.png" alt="Dataset"></p>
<p>Inoltre i video sono stati divisi in modo che i frame di ogni classe fossero distribuiti in modo equo nei diversi fold. La distribuzione dei frame per ogni fold è mostrata nella figura seguente. Gli istogrammi sono stati normalizzati: rappresentano la percentuale di frame per ogni contesto/sottoclasse sul totale per ogni distribuzione riportata. Sono riportati insieme sia il Training Set [TR] che il Test Set [TE].</p>
<p><img src="egonature_images/framePerc.PNG" alt="Istogrammi"></p>
<h2 id="metodi">Metodi</h2>
<p>Il presente progetto si concentra sull'utilizzo di un classificatore basato sul modello ResNet18 per svolgere il task di image classification. ResNet18 è un'architettura di rete neurale convoluzionale molto popolare e ampiamente utilizzata nell'ambito della computer vision. In particolare, la rete è stata pre-addestrata su un vasto dataset di immagini (ImageNet) e utilizzata come punto di partenza per il fine-tuning sullo specifico task di classificazione.</p>
<p>Il classificatore è stato implementato utilizzando il framework PyTorch e PyTorch Lightning. La rete ResNet18 è stata caricata utilizzando la funzione resnet18 di torchvision, che restituisce una istanza della classe ResNet con i pesi pre-addestrati. Successivamente, il classificatore è stato modificato sostituendo l'ultimo layer fully-connected con un nuovo layer lineare che restituisce il numero di classi specifico per la modality scelta.</p>
<p>Il dataset era gia stato diviso in precedenza in training e test set, e successivamente in fold. Il set di training è stato utilizzato per l'addestramento del classificatore tramite l'ottimizzazione di una loss function, in particolare la Cross-Entropy Loss. Data la separazione in fold del dataset si è quindi deciso di usare una 3-fold cross validation, presentando quindi l'accuracy media fra i tre modelli. Il set di test è stato utilizzato per la valutazione delle prestazioni finali del classificatore.</p>
<p>L'ottimizzazione dei parametri del classificatore è stata eseguita tramite la discesa del gradiente stocastico utilizzando l'algoritmo Adam con il learning rate specificato come iperparametro. L'addestramento della rete è stato effettuato per un numero fissato di epoche.</p>
<h3 id="resnet">ResNet</h3>
<p>L'architettura ResNet (He, K., Zhang, X., Ren, S., &amp; Sun, J. <a href="https://arxiv.org/abs/1512.03385">2015</a>)  (Residual Network) è stata introdotta nel 2015 e rappresenta un'evoluzione delle classiche architetture di reti neurali convoluzionali. Il principale vantaggio di ResNet rispetto ad altre architetture è l'utilizzo di blocchi residui, ovvero di un'architettura a &quot;skip connections&quot; che permette di superare i problemi di degradazione della performance che si verificano in presenza di reti molto profonde.</p>
<p><img src="resnet_images/residual.png" alt=""></p>
<p>In particolare, i blocchi residui permettono al flusso di informazione di &quot;saltare&quot; alcuni layer della rete e andare direttamente a layer successivi, senza dover passare attraverso tutti i layer intermedi. Questo aiuta a evitare problemi di vanishing gradient, che si verificano quando la derivata della funzione di attivazione si avvicina a zero e rallenta il processo di apprendimento.</p>
<p>In ResNet, i blocchi residui sono composti da due strati convoluzionali e da un'operazione di somma tra l'input e l'output del blocco. In questo modo, il flusso di informazione viene diviso in due parti: la parte che rappresenta l'input originale e la parte che rappresenta l'aggiornamento delle feature map tramite i due strati convoluzionali.</p>
<p><img src="resnet_images/ResNetArchitecture.png" alt=""></p>
<p>Di ResNet esistono varie versioni, che si distinguono l'una dall'altra per la la complessità del modello, ciascuna identificata dal numero di layer che solitamente accompagnano il nome del modello (ad esempio ResNet18, ResNet34, ResNet50, ResNet101, ResNet152). In particolare, ResNet18 è un modello relativamente semplice che è stato utilizzato come base per il classificatore implementato in questo progetto poichè il task non richiedeva una grande complessità del modello.</p>
<h3 id="transfer-learning-e-fine-tuning">Transfer Learning e Fine Tuning</h3>
<p>Il Transfer Learning è una tecnica di machine learning che consiste nel trasferire la conoscenza appresa da un modello ad un altro modello. In particolare, il modello che viene utilizzato come punto di partenza è un modello pre-addestrato su un dataset di immagini molto grande, come ad esempio ImageNet. Questo permette di utilizzare le conoscenze apprese dal modello pre-addestrato per risolvere un task diverso da quello per cui il modello è stato originariamente addestrato.</p>
<p>Il Fine Tuning è una tecnica di Transfer Learning che si concentra sull'adattamento di una rete neurale già addestrata, ottimizzando tutti i suoi pesi, al fine di migliorare la sua capacità di classificazione su un nuovo insieme di dati specifici. In particolare, si utilizza una rete neurale già addestrata come base, si rimuove il classificatore finale e si sostituisce con uno nuovo specifico al nuovo task.</p>
<p>In generale, il Fine Tuning permette di adattare una rete neurale che ha dimostrato una buona capacità di classificazione su un ampio insieme di dati, per un nuovo compito di classificazione specifico. Ciò consente di evitare l'onere computazionale dell'addestramento di una rete neurale da zero, riducendo il tempo di sviluppo del modello e migliorando l'efficienza dell'addestramento.</p>
<h2 id="valutazione">Valutazione</h2>
<p>Per la valutazione dei modelli si è scelto di utilizzare la metrica di accuracy, metrica standard per la valutazione di modelli di classificazione ed anche la metrica utilizzata nel paper di riferimento.</p>
<p>Inoltre, si è scelto di utilizzare la 3-fold cross validation per la valutazione dei modelli, in quanto il dataset è relativamente piccolo e la 3-fold cross validation permette di ottenere una stima più accurata delle prestazioni del modello.</p>
<p>Successivamente sono state anche calcolate le matrici di confusione per ogni modello, in modo da poter evidenziare eventuali errori di classificazione.</p>
<p>Infine sono state calcolate le metriche di precision, recall e f1-score per ogni classe.</p>
<h2 id="esperimenti">Esperimenti</h2>
<p>Come accennato in precedenza sono stati effettuati tre set di esperimenti, uno per ogni modality. Per ogni modality sono stati quindi effettuati tre training separati, uno per ogni fold.</p>
<p>In ogni esperimento si è utilizzato ResNet18 con learning rate pari a 1e-3, ottimizzando la Cross-Entropy Loss tramite l'algoritmo Adam.</p>
<p>L'addestramento è stato effettuato per 50 epoche in tutte le modality.</p>
<p>Dipendentemente dalla modality, l'ultimo layer della rete è stato modificato per adattarlo al numero di classi del dataset. In particolare, nel caso di modality Context e Subcontext, il numero di classi è pari a 9, mentre nel caso di modality ContextSubcontext è pari a 17.</p>
<p>In fase di test è stata computata la confusion matrix, ovvero una matrice che mostra il numero di predizioni corrette e incorrette fatte dal modello rispetto al numero totale di predizioni per ogni classe.</p>
<div style="display: flex; flex-direction: row; justify-content: space-between">
    <img src="confusion_matrixes/confusion_matrixCon_fold012.png" width="50%">
    <img src="confusion_matrixes/confusion_matrixSub_fold012.png" width="50%">
</div>
<div style="display: flex; flex-direction: row; justify-content: space-between">
    <img src="confusion_matrixes/confusion_matrixConSub_fold012.png" width="100%">
</div>
<br/>
<p>Inoltre sono state calcolate accuracy
medie per ogni classe, ovvero la percentuale di predizioni corrette fatte dai modello per ogni classe rispetto al numero totale di predizioni fatte dai modelli per quella classe.</p>
<p>Di seguito una tabella con l'accuracy media per ogni modality</p>
<p style="text-align: center;">Accuracy</p>
<div align="center">
<table>
<thead>
<tr>
<th>Modality</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Context</td>
<td>89.59%</td>
</tr>
<tr>
<td>Subcontext</td>
<td>86.01%</td>
</tr>
<tr>
<td>ContextSubcontext</td>
<td>85.72%</td>
</tr>
</tbody>
</table>
</div>
<p>Di seguito una tabella con la precision/recall media per ogni classe per ogni modality</p>
<p style="text-align: center;">Con</p>
<div align="center">
<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Entrance</td>
<td>0.825</td>
<td>0.929</td>
<td>0.873</td>
</tr>
<tr>
<td>Monumental Building</td>
<td>0.892</td>
<td>0.842</td>
<td>0.865</td>
</tr>
<tr>
<td>Greenhouse</td>
<td>0.935</td>
<td>0.976</td>
<td>0.955</td>
</tr>
<tr>
<td>Succulents</td>
<td>0.936</td>
<td>0.949</td>
<td>0.943</td>
</tr>
<tr>
<td>Sicilian Garden</td>
<td>0.958</td>
<td>0.948</td>
<td>0.953</td>
</tr>
<tr>
<td>Leftmost Garden</td>
<td>0.797</td>
<td>0.687</td>
<td>0.732</td>
</tr>
<tr>
<td>Passageway</td>
<td>0.783</td>
<td>0.628</td>
<td>0.696</td>
</tr>
<tr>
<td>Central Garden</td>
<td>0.884</td>
<td>0.803</td>
<td>0.840</td>
</tr>
<tr>
<td>Rightmost Garden</td>
<td>0.772</td>
<td>0.528</td>
<td>0.616</td>
</tr>
</tbody>
</table>
</div>
<p style="text-align: center;">Sub</p>
<div align="center">
<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dune</td>
<td>0.876</td>
<td>0.825</td>
<td>0.847</td>
</tr>
<tr>
<td>Dune-Scrubland</td>
<td>0.848</td>
<td>0.846</td>
<td>0.847</td>
</tr>
<tr>
<td>Marshy-Forestry</td>
<td>0.827</td>
<td>0.842</td>
<td>0.833</td>
</tr>
<tr>
<td>Marshy-Coastal</td>
<td>0.749</td>
<td>0.767</td>
<td>0.757</td>
</tr>
<tr>
<td>Marshy</td>
<td>0.921</td>
<td>0.796</td>
<td>0.854</td>
</tr>
<tr>
<td>Coastal</td>
<td>0.906</td>
<td>0.947</td>
<td>0.925</td>
</tr>
<tr>
<td>Cliffs</td>
<td>0.897</td>
<td>0.898</td>
<td>0.897</td>
</tr>
<tr>
<td>Cliffs-Forestry</td>
<td>0.752</td>
<td>0.694</td>
<td>0.706</td>
</tr>
<tr>
<td>Entrance</td>
<td>0.820</td>
<td>0.844</td>
<td>0.832</td>
</tr>
</tbody>
</table>
</div>
<p style="text-align: center;">ConSub</p>
<div align="center">
<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Entrance</td>
<td>0.822</td>
<td>0.923</td>
<td>0.869</td>
</tr>
<tr>
<td>Monumental Building</td>
<td>0.856</td>
<td>0.858</td>
<td>0.856</td>
</tr>
<tr>
<td>Greenhouse</td>
<td>0.935</td>
<td>0.976</td>
<td>0.954</td>
</tr>
<tr>
<td>Succulents</td>
<td>0.923</td>
<td>0.958</td>
<td>0.940</td>
</tr>
<tr>
<td>Leftmost Garden</td>
<td>0.820</td>
<td>0.655</td>
<td>0.724</td>
</tr>
<tr>
<td>Passageway</td>
<td>0.738</td>
<td>0.623</td>
<td>0.675</td>
</tr>
<tr>
<td>Central Garden</td>
<td>0.863</td>
<td>0.813</td>
<td>0.837</td>
</tr>
<tr>
<td>Rightmost Garden</td>
<td>0.699</td>
<td>0.573</td>
<td>0.625</td>
</tr>
<tr>
<td>Dune</td>
<td>0.884</td>
<td>0.679</td>
<td>0.766</td>
</tr>
<tr>
<td>Dune-Scrubland</td>
<td>0.846</td>
<td>0.843</td>
<td>0.844</td>
</tr>
<tr>
<td>Marshy-Forestry</td>
<td>0.799</td>
<td>0.799</td>
<td>0.799</td>
</tr>
<tr>
<td>Marshy-Coastal</td>
<td>0.739</td>
<td>0.726</td>
<td>0.729</td>
</tr>
<tr>
<td>Marshy</td>
<td>0.865</td>
<td>0.753</td>
<td>0.798</td>
</tr>
<tr>
<td>Coastal</td>
<td>0.902</td>
<td>0.867</td>
<td>0.883</td>
</tr>
<tr>
<td>Cliffs</td>
<td>0.761</td>
<td>0.860</td>
<td>0.804</td>
</tr>
<tr>
<td>Cliffs-Forestry</td>
<td>0.644</td>
<td>0.609</td>
<td>0.607</td>
</tr>
<tr>
<td>Entrance_Sicilian</td>
<td>0.757</td>
<td>0.687</td>
<td>0.720</td>
</tr>
</tbody>
</table>
</div>
<p>Infine sono state calcolati i tempi di inferenza e il peak usage della memoria RAM;</p>
<div align="center">
<table>
<thead>
<tr>
<th>Inference Time</th>
<th>Peak RAM Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>852.66 ms</td>
<td>55.77 MB</td>
</tr>
</tbody>
</table>
<p>Tutti gli esperimenti sono stati svolti con una RTX 3060ti con 8GB di VRAM e 16GB di RAM.</p>
</div>
<h2 id="demo">Demo</h2>
<p>Per testare le funzionalità del progetto è stato creato uno script <code>demo.py</code> che permette di testare un qualunque modello su un'immagine a scelta, specificando il path dell'immagine, path del modello e la modality.</p>
<p>Lo script richiede i seguenti dati:</p>
<pre class="hljs"><code><div>usage: py demo.py [-h] -m MODEL -i INPUT -mod {Con,Sub,ConSub}

Utility Script to use the model trained for the ML project on the EgoNature Dataset

options:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        model path (default: None)
  -i INPUT, --input INPUT
                        input image path (default: None)
  -mod {Con,Sub,ConSub}, --modality {Con,Sub,ConSub}
                        modality (default: None)
</div></code></pre>
<p>Lo script è anche in grado di recuperare la label di ground truth dell'immagine, se questa fa parte del test set originale.
Un example usage dello script:</p>
<pre class="hljs"><code><div>py demo.py -m best_models/resnet18_ConSub_fold2.ckpt -i data/EgoNature-Dataset/EgoNature-Dataset/EgoNature\ -\ CNN/2018_02_21_000_0000059.jpeg -mod ConSub
</div></code></pre>
<p>Lo script stamperà sullo standard output la probabilità di appartenenza dell'immagine a ciascuna classe e la classe di ground truth, se disponibile, insieme al tempo di inferenza e al peak usage della memoria RAM.
Un esempio di output è il seguente:</p>
<pre class="hljs"><code><div>Elapsed time: 861.23 ms
Peak memory usage: 55.77 MB
----------------------------------------
Entrance: 0.07%
Monumental Building: 0.00%
Greenhouse: 0.00%
Succulents: 0.00%
Leftmost Garden: 0.03%
Passageway: 0.00%
Central Garden: 0.02%
Rightmost Garden: 0.00%
Dune: 99.41% &lt;--- True label
Dune-Scrubland: 0.07%
Marshy-Forestry: 0.00%
Marshy-Coastal: 0.10%
Marshy: 0.00%
Coastal: 0.00%
Cliffs: 0.27%
Cliffs-Forestry: 0.03%
Entrance_Sicilian: 0.00%
</div></code></pre>
<p>Inoltre viene generata, all'interno della cartella results, un'immagine con matplot con una piccola rappresentazione della probabilità di appartenenza dell'immagine a ciascuna classe; di seguito quattro esempi di output, due corretti e due errati:</p>
<div style="display: flex; flex-direction: row; justify-content: space-between">
    <img src="results/results_good1.png" width="50%">
    <img src="results/results_good2.png" width="50%">
</div>
<div style="display: flex; flex-direction: row; justify-content: space-between">
    <img src="results/results_bad1.png" width="50%">
    <img src="results/results_bad2.png" width="50%">
</div>
<br/>
<h2 id="codice">Codice</h2>
<p>Il codice è stato scritto in Python e utilizza le seguenti librerie:</p>
<ul>
<li>PyTorch</li>
<li>PyTorch Lightning</li>
<li>Torchvision</li>
<li>Numpy</li>
<li>Matplotlib</li>
<li>Pandas</li>
<li>Pillow</li>
</ul>
<p>All'interno della root del progetto sono presenti i seguenti file:</p>
<ul>
<li>
<p><code>settings.py</code>: contiene le variabili di configurazione del progetto</p>
</li>
<li>
<p><code>egonature.py</code>: contiene le classi <code>EgoNatureDataset</code> e <code>EgoNatureDataModule</code> che permettono di caricare il dataset per l'addestramento dei modelli.</p>
</li>
<li>
<p><code>classifier.py</code>: contiene la classe <code>ResNet18Classifier</code> che permette di definire la rete neurale ResNet18 e il classificatore finale.</p>
</li>
<li>
<p><code>train.py</code>: contiene la logica di addestramento della rete; accetta un parametro fra <code>[0, 1, 2]</code> che specifica quale fold utilizzare.</p>
</li>
<li>
<p><code>confusion.py</code>: contiene la logica per il calcolo della confusion matrix.</p>
</li>
<li>
<p><code>demo.py</code>: contiene la logica per il testing di un modello su un'immagine a scelta, come spiegato nella sezione precedente.</p>
</li>
<li>
<p><code>best_models</code>: contiene i modelli migliori ottenuti durante gli esperimenti.</p>
</li>
<li>
<p><code>data</code>: contiene il dataset, se non presente verrà creata al primo avvio di <code>train.py</code>.</p>
</li>
<li>
<p><code>metrics.py</code>: contiene la logica per il calcolo delle metriche di precision, recall e accuracy, crea inoltre un file csv contenente le metriche per il fold specificato.</p>
</li>
</ul>
<p>Per poter utilizzare il progetto è ovviamente necessario lanciare inizialmente la fase di training, specificando opportunamente la path di salvataggio del checkpoint, la modality da utilizzare e il fold da utilizzare. Ad esempio:</p>
<pre class="hljs"><code><div>py train.py -m best_models -mod Sub -f 0
</div></code></pre>
<p>Una volta addestrato il modello, è possibile calcolare la confusion matrix e l'accuracy tramite il comando:</p>
<pre class="hljs"><code><div>py confusion.py -m best_models -mod Sub -f 0
</div></code></pre>
<p>Di default calcola la matrice aggregata fra tutti i fold della modality, altrimenti si può specificare un fold attraverso l'opzion <code>-f</code>. Verrà quindi generato un file png contenente la confusion matrix e verrà stampata l'accuracy media.</p>
<p>E' possibile ottenerne le metriche di precision, recall ed F1 per ogni classe tramite il comando:</p>
<pre class="hljs"><code><div>py metrics.py -m best_models -mod ConSub
</div></code></pre>
<p>N.B. il comando non accetta l'opzione <code>-f</code> in quanto calcola le metriche aggregando i risultati di tutti i fold, per tale ragione è necessario che tutti i fold siano stati addestrati.</p>
<p>Infine, è possibile testare il modello su un'immagine a scelta tramite il comando:</p>
<pre class="hljs"><code><div>py demo.py [-h] -m MODEL -i INPUT -mod {Con,Sub,ConSub}
</div></code></pre>
<p>dove <code>MODEL</code> è il path del modello, <code>INPUT</code> è il path dell'immagine e <code>modality</code> è la modality da utilizzare.</p>
<h2 id="conclusioni">Conclusioni</h2>
<p>In conclusione, il progetto ha permesso di ottenere dei risultati interessanti in termini di accuratezza utilizzando un approccio di transfer learning basato su ResNet18 per risolvere il problema della localizzazione di visitatori in ambienti naturali.</p>
<p>L'utilizzo di ResNet18 come base di partenza ha permesso di ridurre notevolmente il tempo di addestramento e lo sforzo computazionale, trasferendo la conoscenza pregressa accumulata su ImageNet.</p>
<p>Il codice sviluppato rende facile l'addestramento di nuovi modelli, il calcolo di metriche e il testing su nuove immagini, pur rimanendo focalizzati sul task specifico.</p>
<p>In definitiva, i risultati ottenuti dimostrano come un approccio di transfer learning e fine tuning possa permettere di risolvere in modo efficace problemi di classificazione su dataset ridotti.</p>

</body>
</html>
